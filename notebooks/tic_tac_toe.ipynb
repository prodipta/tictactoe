{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The associated codes to run this notebook can be found [here](https://github.com/prodipta/tictactoe). Clone the repo, and open this notebook (notebooks/tic_tac_toe.ipynb).\n",
    "\n",
    "## Reinforcement Learning: An Introduction\n",
    "\n",
    "Reinforcement learning is a machine learning algorithm for general purpose decision making. But before we understand and appreciate this, let's review our decision making process\n",
    "\n",
    "### Exhibit 1: Evaluating decisions based on outcome\n",
    "\n",
    "\n",
    "`World T20 Final 2007`: Ace spinner **Harbhajan Singh** had an over left but Dhoni chose medium-pacer **Joginder Sharma** to bowl the last over. **Misbah-ul-Haq** was strong at crease.\n",
    "\n",
    "<img src=\"resources/dhoni_happy.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "\n",
    "`World Cup 2011 India vs South Africa`: Ace spinner **Harbhajan Singh** had an over left but Dhoni chose medium-pacer **Ashish Nehra** to bowl the last over. **Robin Peterson** was strong at crease.\n",
    "\n",
    "<img src=\"resources/dhoni_sad.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Sports critics and media absolutely loved the first decision, and similarly totally rejected the second one.\n",
    "\n",
    "\n",
    "Too often we evaluate our past decisions based on the outcomes. We completely forget the situations we were in when the decisions were taken, and the information we had till at the point. The above two decisions are, arguably, very similar. They had very different outcome. And were judged very differently by the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhibit 2: Short Term or Long term Rewards?\n",
    "\n",
    "`The Standford Marshmallow Experiment`: The [experiment](https://en.wikipedia.org/wiki/Stanford_marshmallow_experiment#Original_Stanford_experiment) was first carried out in 1972 in Standford University. 32 children, under isolation with little distractions, were offered a treat (cookies or five pretzel sticks). The researchers explained to the children that they could eat the treat now and leave the room. But if they waited 15 minutes without eating the immedeate treat, they would get another.\n",
    "\n",
    "<img src=\"resources/marshmallow.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Often the rewards are realized in the future, but we need to act now. This is known as `delayed gratification` in economics and behavioural science. The above is a simple two period case. In real life, everyday we face a continuous choice of multi-period version of this game\n",
    "\n",
    "- Stock has rallied (sold-off) significantly, shall I sell now at a small profit (loss) now, or wait for larger profit in future?\n",
    "- Shall I drink 10 rupess tea now, or let go for a month and pay off my loan shark?\n",
    "\n",
    "Since rewards can realize over different time horizon, we need a way to compare them consistently **now**. A standard way of doing it is through discounting (usually time-consistent discounting i.e. `exponential` discounting, but in real life we also see `hyperbolic` discounting). This is also known as time-preference or temporal preference in economics or behavioural science. This time discouting is the bedrock of all our financial system - NPV\n",
    "\n",
    "- bonus: what is the fundamental underlying assumption for discounting in financial markets (e.g. asset pricing, discount cashflow valuation, NPV project valuation etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhibit 3: Risk vs Reward\n",
    "\n",
    "You are managing a risky project<sup>[1]</sup> at work. If it works out well, you can get a good promotion. The chances are, if it gets derailed, you may get sidelined. How you make your decisions?\n",
    "\n",
    "<img src=\"resources/risky.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "- In a knock-out round of a poker tournament, do you jam your small pocket pairs against a button raiser? What if you go bust?\n",
    "- how to balance risk vs. reward\n",
    "\n",
    "The balancing of risk and rewards under uncertainty is another major criteria for a good decision making process\n",
    "\n",
    "### Exhibit 4: Multi-armed bandits\n",
    "\n",
    "A gambler is deciding to play a row of slot machines - each with unknown, but different characteristics (win probability, payout etc.). The problem is how to distribute time and capital among different machines to try and find the best. This is a classifc decision making problem - exploration vs. exploitation\n",
    "\n",
    "<img src=\"resources/bandits.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "- To settle down with the current person you are dating or look for another?\n",
    "- To select the current candidate who is good enough, or interview more for a better fit\n",
    "\n",
    "### How to make a good decisions\n",
    "\n",
    "- Evaluate (both prospectively and retrospectively) your decisions based on `information you have at the time of decision`\n",
    "- For multi-period delayed rewards, define a `discouting function` to be able to compare and chose temoporally distributed rewards\n",
    "- For uncertain environments, define a `reward function` to balance risk and reward.\n",
    "- For uncertain environments, device a strategy to balance `exploration` against `exploitation`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Problem Statement\n",
    "\n",
    "You are interacting with a `system`. At each `state` of the system, you must choose an allowed `action`. After each such `action`, you may or may not receive an immediate `reward` (or penalty, a negative reward). After an uncertain n `steps` (or `state` changes), the `system` reaches its final `state` and you realize the final `reward` (or penalty).\n",
    "\n",
    "- In financial investing, the `system` is the market itself. Your `action` is either buy, sell or hold of the stocks in the universe, and rewards are profit and loss.\n",
    "\n",
    "- In a game of poker, the `system` is your opponents collectively, (only one in heads-up poker, a mathematically easier version). Your `actions` are fold, bet, call, raise or re-raise. Your rewards are realized only after each hands (or at the end of the whole game if it is a tournament)\n",
    "\n",
    "- In a game of chess, the `system` is the board and the opponent. The `state` is the current configuration of the board. Your `actions` are defined by the chess rules and your remaining chess pieces. The `reward` is received only when the game is over.\n",
    "\n",
    "All these decision making problems are similar in nature, although they vary greatly with each parameters - `state`, `actions` and `rewards`. \n",
    "\n",
    "For example, in tic-tac-toe, the `state` are finite and relatively small. In chess, `states` are much more numerous, but they are still somewhat manageable. In poker, `states` are never known fully and can only be guessed - you do not get to see your opponents hand. In markets `state` is infinite - an endless combinations of varius market factors, prices and sentiments etc.\n",
    "\n",
    "We aim to find a generalized approach to solve such problems. We try to crack the tic-tac-toe game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe: A demo for reinforcement learning\n",
    "\n",
    "from [wikipedia](https://en.wikipedia.org/wiki/Tic-tac-toe): Tic-tac-toe (American English), noughts and crosses (Commonwealth English), or Xs and Os is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3Ã—3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner.\n",
    "\n",
    "### The decision making problem\n",
    "\n",
    "Suppose we start with the 'X'. The decision problem is choose a series of action that leads to a potential win. The `reward` - win, loss, or a draw, comes at the end of the game. To get there, we need to make a series of decisions where we do not get immedeate clue if our decision was correct or wrong. What are the way(s) to create an algorithm to win?\n",
    "\n",
    "### A common man's approach\n",
    "\n",
    "Start with a random choice, say top corner. Then, each time it is our turn to play, do the following in order:\n",
    "\n",
    "- block an impending triad completion of our opponent, if any\n",
    "- Else, we put a 'X' to that makes `possible for us to complete a triad in the next move` in case our opponent does not notice it.\n",
    "\n",
    "This is probably be a decent strategy for a beginner.\n",
    "\n",
    "### A mathametical approach\n",
    "\n",
    "Proposed in 1972, by [Newell and Simon](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy). It is a collection of heuristics that relies on combinatorics. The [game complexity](https://en.wikipedia.org/wiki/Game_complexity#:~:text=For%20tic%2Dtac%2Dtoe%2C,have%20a%20row%20of%20three.) is relatively mild - total 19,683 possible configuration - reduces drastically if we remove invalid configurations (e.g.  five crosses and no noughts).\n",
    "\n",
    "### What about variations\n",
    "\n",
    "This strategies quickly become more complicated as we expand the game - say `nxn` board with first `k` to finish is a winner. It can also be complicated, e.g. instead of only two characters what if we have 8, each with different rules (chess!). Or say the two players are playing on private boards not visible to each other (not a very good example in this case, these types of games are called `incomplete information`).\n",
    "\n",
    "### Can we come up with something more general\n",
    "\n",
    "Remeber the theme of `ML and AI`. We do not want to solve each problems with specialized algorithms. Rather we want to use standardized algorithms to solve many problems (of similar class) with different inputs. And this class of problems are general decision making problems with delayed reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one-size-fits-all ML pipeline\n",
    "\n",
    "What if we use the standardized ML approach. We generate random simulations for many many games and tabulate each board configuration and the action taken, and label it if it was positive or negative. Then we use this labelled dataset to fit a supervised model. Then use the model to decide for us.\n",
    "\n",
    "<img src=\"resources/ml_workflow.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "### Design questions\n",
    "\n",
    "- how to capture the data? How we capture the board configuration (it is a 3X3 matrix)\n",
    "- how to label the data? We do not know the immedeate reward at each action step\n",
    "- how to select features?\n",
    "- how to choose the algorithm? What should be a good loss function? What should be the estimator (that fits the data to the observations, e.g. a `random forest`, or `svm` model or a `deep NN`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Input data\n",
    "\n",
    "How can we generate input data for our current problem? If we know what data we need, we can look for a vendor that supplies such data. What data we need?\n",
    "\n",
    "### What data we need\n",
    "\n",
    "For our problem we need labelled set of data that tells us what was the board configuration at a certain step when we had the turn, what actions we took, and what was the reward. No one unfortunately sells such data. The good news is, we can easily generate it ourselves by simulation\n",
    "\n",
    "### Developing the Simulation\n",
    "\n",
    "To simulate a tic-tac-toe game, we can code the game in a Python class. The features that we are looking for is something like below:\n",
    "\n",
    "```python\n",
    "class Board():\n",
    "    \"\"\"\n",
    "        A board class to simulate our tic-tac-toe game. We need to identify the players, say 'X' is the first player, 'O' is the second player.\n",
    "    \"\"\"\n",
    "    def __init__(self, player1='X', player2='O', blank=' '):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        \"\"\" returns the current configuration of the board. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def winner(self):\n",
    "        \"\"\" returns the winner if any, or `None`. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def moves(self):\n",
    "        \"\"\" returns the current available moves left in the board.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def game_over(self):\n",
    "        \"\"\" if the game is over. \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def step(self, player, move):\n",
    "        \"\"\" simulate the game, taking input from `player` and generating random move from the other player. \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "We have already coded such a class in the `board.py` file. So let's import and simulate games to generate some data. We need to store the input board configuration, the action and the reward. The board can be represented as a three lines of 3 cells each. We will store the configuration as `flattened` version of this 3x3 matrix (so 3x3 matrix becomes a 1x9 vector). Similarly, action identifies a particular cell to mark - it is a combination of a (row, column) `tuple`. We will store this as a 1x2 vector. So these two stacked together horizontally, our features vector become 1x11.\n",
    "\n",
    "For the rewards, we will mark +100 if we won in that step, -100 if we lost in that step, otherwise (game not over, or it is a draw) we will mark 0.\n",
    "\n",
    "Let's see the simulation code. We start with setting our paths and importing the classes we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import_path = os.path.abspath(os.path.join('..'))\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tictactoe.board import Board\n",
    "from tictactoe.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play some games\n",
    "\n",
    "Let's simulate some games using the `Board` class we just generated. Below we instantiate a `Board` object (the `board` variable). We choose to play player `X` (first mover). Then we `step` through the board simulation till the game is over.\n",
    "\n",
    "In the next cell, we generate the data as we discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the winner is O\n",
      "\n",
      "  X  |  X  |  O  \n",
      "-----+-----+-----\n",
      "  X  |  O  |  O  \n",
      "-----+-----+-----\n",
      "     |  X  |  O  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "board = Board('X','O', ' ')\n",
    "player = 'X'\n",
    "while not board.game_over:\n",
    "    board.step(player, None)\n",
    "    \n",
    "print('the winner is {}\\n'.format(board.winner))\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick benchmark\n",
    "\n",
    "Before we start creating the data and model for our ML tic-tac-toe player, let's establish a benchmark to evaluate it against. If we play randomly (and the board plays against us randomly as well), we should have approx 1/3 changes of win, losses and draws. Since we are the first-mover, we will have slight advantage in a game like tic-tac-toe (and chess and similar full-information games). Let's simulate a games and score it when both players play randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.35 0.32\n"
     ]
    }
   ],
   "source": [
    "## CAUTION: it may take a while to run, we simulate a large number of games\n",
    "\n",
    "player = 'X'\n",
    "N = 10000\n",
    "wins = draws = losses = 0\n",
    "        \n",
    "for i in range(N):\n",
    "    board = Board('X','O', ' ')\n",
    "    if player != 'X':\n",
    "        board.make_random_move()\n",
    "\n",
    "    while not board.game_over:\n",
    "        choice = board.random_move()\n",
    "        _, reward, terminated = board.step(player, choice)\n",
    "        if terminated:\n",
    "            if reward == 0:\n",
    "                draws = draws + 1\n",
    "            elif reward > 0:\n",
    "                wins = wins + 1\n",
    "            else:\n",
    "                losses = losses + 1\n",
    "                    \n",
    "print(round(wins/N,2), round(draws/N,2), round(losses/N,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the random bench mark gives us a slight advantange (33% win against 32% loss). Let's see if we can do better with our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is of shape (10000, 11), y is of shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "### Generate the data\n",
    "player = 'X'\n",
    "N = 10000\n",
    "X = np.zeros((N,(9+2)))\n",
    "y = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    board = Board('X','O', ' ')\n",
    "    if player != 'X':\n",
    "        board.make_random_move()\n",
    "\n",
    "    while not board.game_over:\n",
    "        config = np.zeros(board.board.shape)\n",
    "        config[board.board=='X'] = 1\n",
    "        config[board.board=='O'] = 0\n",
    "        config[board.board==' '] = -1\n",
    "        action = board.random_move()\n",
    "        _, reward, _ = board.step(player, action)\n",
    "        X[i,0:9] = config.flatten()\n",
    "        X[i,9:12] = np.array(action).flatten()\n",
    "        y[i] = np.sign(reward)\n",
    "        \n",
    "print(f'X is of shape {X.shape}, y is of shape {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "Next step is to create the model. Notice we have a `categorical` reward function. The rewards 100, -100 or 0 probably indicates levels (or classes) rather than actual (monetary) value. If this assumption is true, we have a case of `classification` problem at hand. We abhor losses more than anything, so we mark negative reward as -1, and a win or a draw is +1. This is a case of `binary classification`.\n",
    "\n",
    "We choose a [binary cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss function, along with a `Random Forest` classifier. We could also have chosen a `DNN`. We split the whole data set to 70-30 as our train vs. test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6326666666666667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Now that we have our model ready, we can run game scenarios to evaluate. We simulate each game to completion, and each turn, evaluate the predicted reward based on all possible actions (the last two in the features columns) and choose the one that does the best. Then over `N` different games we score how many we lost, won or drew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56 0.27 0.17\n"
     ]
    }
   ],
   "source": [
    "## CUATION: This may take a long time to run!\n",
    "\n",
    "player = 'X'\n",
    "N = 100\n",
    "wins = draws = losses = 0\n",
    "        \n",
    "for i in range(N):\n",
    "    board = Board('X','O', ' ')\n",
    "    if player != 'X':\n",
    "        board.make_random_move()\n",
    "\n",
    "    while not board.game_over:\n",
    "        config = np.zeros(board.board.shape)\n",
    "        config[board.board=='X'] = 1\n",
    "        config[board.board=='O'] = 0\n",
    "        config[board.board==' '] = -1\n",
    "        \n",
    "        moves = board.moves\n",
    "        choice = board.random_move()\n",
    "        features = np.hstack([config.flatten(), np.array(choice).flatten()])\n",
    "        best = rf.predict(features.reshape(1,11))\n",
    "        for move in moves:\n",
    "            features = np.hstack([config.flatten(), np.array(move).flatten()])\n",
    "            predict = rf.predict(features.reshape(1,11))\n",
    "            if predict > best:\n",
    "                best = predict\n",
    "                choice = move\n",
    "                \n",
    "        _, reward, terminated = board.step(player, choice)\n",
    "        if terminated:\n",
    "            if reward == 0:\n",
    "                draws = draws + 1\n",
    "            elif reward > 0:\n",
    "                wins = wins + 1\n",
    "            else:\n",
    "                losses = losses + 1\n",
    "                    \n",
    "print(round(wins/N,2), round(draws/N,2), round(losses/N,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely better. But is it good enough? Can we do better?\n",
    "\n",
    "## What is the problem here?\n",
    "\n",
    "It is not nearly good enough. Given the fact that our opponent is playing randomly, we expected to do a lot better than just 56% wins! The main problem here is our decisions are quite `myopic`, we have trained the model thus. It is not a good way to solve decision problems with delayed rewards. All our input samples were independentl. But in reality, our decisions form a chain and the reward is at the end of it. All actions in a particular game are related. We failed to capture that. This motivates us to look at these decision making problems differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision making framework\n",
    "\n",
    "We take a fresh approach to solve this problem. Recall our three points in good decision making: \n",
    "\n",
    "- we take decision based on the currently available information\n",
    "- we use discounting to normalize delayed rewards \n",
    "- we use risk-reward balance for risky games and\n",
    "- we follow a strategy to balance exploitation vs. exploration\n",
    "\n",
    "What should be our objective for decision making\n",
    "\n",
    "### Maximize the value of our decision\n",
    "\n",
    "One good approach is to choose a series of `actions` that try and maximize the expected `reward` (expected immediate and future rewards based on current information, or mathematically, conditioned on current `state`)\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s) = \\mathbb E[R_{0}(s_{0},a_{0}) + \\gamma.R_{1}(s_{1},a_{1}) + \\gamma^2.R_{2}(s_{2},a_{2}) ....|s_{0}=s,\\pi]\n",
    "\\end{align}\n",
    "\n",
    "Or in other words\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s) = \\mathbb E\\Bigl[\\sum_n(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "We aim to find `policy` such that we maximize this expression. One issue with the above expression is that it tells us what is the expected worth of our actions, but does not tell us what to do. So let's recast the above such that the `action` is a parameter as well, not just `state`.\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s,a) = \\mathbb E\\Bigl[\\sum_n(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s, a_{0}=a,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "This makes the first term deterministic, so we take this out and the expression becomes\n",
    "\n",
    "\\begin{align}\n",
    "Q_{0}^\\pi(s,a) = R_{0}(s,a) + \\mathbb E\\Bigl[\\sum_{n=1}(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s, a_{0}=a,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "### The Bellman Equation\n",
    "\n",
    "Next, we do two simple changes - first, we change the subscript of the value Q to variable time, to make the equation dynamic explicitly. Then, we notice that the second part of the above equation is just gamma times the value Q itself. So we recast a dynamic version of the above equation as follows:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{t}^\\pi(s,a) = R_{t}(s,a) + \\gamma.Q_{t+1}^\\pi(s,a)\n",
    "\\end{align}\n",
    "\n",
    "From the above, and from Bellman's [principle of optimality](https://en.wikipedia.org/wiki/Bellman_equation), we claim that to maximize the value we must choose Q such that:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{t}^\\pi(s,a) = R_{t}(s,a) + \\gamma.Max(Q_{t+1}^\\pi(s,a))\n",
    "\\end{align}\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "From the above, we get an interesting insight. An algorithm can be developed on the following line:\n",
    "\n",
    "- We create a table (called `Q-table`) that will map our evaluations of each allowed `actions` at each `state`. We start with all zero values.\n",
    "\n",
    "- Next, at initial state, to chose our action `a`, we randomly try all allowed actions and chose the one that provides maximum immediate benifit in the next step.\n",
    "\n",
    "- We remember this favourable action by making an entry in the Q-table, and take the chosen action to move to the next step\n",
    "\n",
    "- Repeat\n",
    "\n",
    "If we do it large enough number of times, we will have enough samples for each `(s,a)` combinations that will allow us average the value computation over and get an expected value. And if these values converge to a limit, we are done. We have created a Q-table, that has seen too many games and memorized all moves to make best judgement best on its memory.\n",
    "\n",
    "What we have done so far is: we have taken care of our first two principles of good decision making process - using the current information to best use, and using discounting function (`gamma`) to compare rewards across time.\n",
    "\n",
    "### The problem - being myopic \n",
    "\n",
    "Notice in the above example we consider only the immediate rewards. Since all actions in a game are connected, that means we may have chosen to leave behind some paths that had low immediate rewards but high final rewards. We were too exploitive and forgot to explore. This risks settling down on a local minima instead of a global one. \n",
    "\n",
    "One way to avoid this is to introduce a new parameter `epsilon`. It is a fraction between 0 to 1. While choosing the next action, now we will not always choose the best on immediate rewards. With a probability of `epsilon`, we will now choose a random action. And only with probability `(1-epsilon)` we will choose the best. This gives us enough randomness in search to explore good paths that reward later. Also, we can have a high value of `epsilon` at the beginning of learning (`explorative`), and reduce (more `expoliting`) towards the end. A popular way to do this is to introduce an exponential decay of `epslion` between each batch of training.\n",
    "\n",
    "Finally, we can also have a parameter `nu` or `learning-rate`, that we use to balance between the old and new values of updates of the Q-table. Instead of overwriting the past learned value each time, we can use a fraction `nu` of the new value and `(1-nu)` of the old value for the final update. This smoothens out our learning process.\n",
    "\n",
    "How to apply all these to our tic-tac-toe problem.\n",
    "\n",
    "\n",
    "### The Game-Agent Model\n",
    "\n",
    "The best way to implement this is the agent mode. We already modelled the game. Now we model an `agent` or a player. We store the Q-table and let the `agent` interact with the environment to learn. In each leaning episode, the `agent` play a complete game, choosing actions based on the algorithm above and updating Q-table at each step. In the next episode, it starts with the existing Q-table and carries out further updates and learnings. Under certain [cicumstances](https://en.wikipedia.org/wiki/Markov_decision_process) this gurantess a convergence. \n",
    "\n",
    "We have already implemented an agent in the file `agent.py`. Let's run a training on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent()\n",
    "a.train(player='X', n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 0.02, 0.08)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a whooping improvement than the last. Our win rate is 90% compared to 56% based on the brute-force machine learning!\n",
    "\n",
    "### So where is the catch!\n",
    "\n",
    "Reinforcement learning is a great method. What we discussed today is a particular version called `Q-learning`. If you followed the algorithm, you would have figured our already where we may have difficulties\n",
    "\n",
    "- First, we made assumption on convergece. This is not a big issue as such and we can always test for it. The problem is as the total `state space` (all possible states or configuration of the game) becomes larger, this also gets more difficult to ascertain\n",
    "\n",
    "- We assumed complete information about the `state space` and current `state`. This is true for tic-tac-toe. This is even true for chess. But in a game like poker, it is difficult to evaluate the states and the rewards as a large and important parts (opponets' hands) are hidden. We have only a limited visibility to the actual `state` at any given point in time.\n",
    "\n",
    "- At least in poker, the states may be hidden, but finite. That is not the case for financial markets. There is an endless possible combinations of market factors, and the `state space` is essentially infinite, in addition of being partially hidden.\n",
    "\n",
    "In such scenarios, we cannot simply update the Q-table following the above method. However, if make parameterized version of it (i.e. moving from a table to a parameterized functions), we can train a ML estimator (like a deep neural network) learn the parameters and predict the Q-value updates.\n",
    "\n",
    "If these predictions are good enough, we can then follow the rest of the algorithm to develop our decision bot. When we combine the `DNN` with `Q-learning`, we get what is known as `Deep Q-learning`.\n",
    "\n",
    "\n",
    "### The butterfly effect\n",
    "\n",
    "<img src=\"resources/butterfly.jpg\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Note, we cannot pick and fit any ML model to parametrize the Q-values. If a linear parametrization fits the bill, it is all good. But for delayed rewards scenario, they will hardly do. Using non-linear parameterization can cause stability issues.\n",
    "\n",
    "Q-learning is essentially operates on a dynamic system. A characteristic of non-linear dynamic program is what is called chaos. A small change in Q can significantly change subsequent learnings and policy. This can be due to correlations between observations, and that between Q and the target (we are learning both, in a way). This can also be due to inherent random noise in a noise system (a random poker player, and of course, the market.\n",
    "\n",
    "### Tic-tac-toe vs Trading\n",
    "\n",
    "As we noted, application of Q-learning or reinforcement learning in general, in finance is possible but not straight forward. \n",
    "\n",
    "The first thing we need to do is to figure out a way to represent our `state space`. We can never have a complete description, like in tic-tac-toe. But we can choose a set of features (say, price moves, technical indicators and fundamental ratios, sentiments and economic data) to represent the market set. \n",
    "\n",
    "The second thing we need to do is to define a reward function. Playing tic-tac-toe is arguably not a risky business. And games may last a few rounds. So we had a low discount rate, and considered only the rewards, not any risk (say, variance in rewards). In finance, doing that we can go bust. So it is very important to design a proper reward function. Possibly candidates are percentage returns (no risk consideration), Sharpe ratio, Return to variance ration (Kelly ratio) etc.\n",
    "\n",
    "Finally, we need to define what is an episode, i.e. what completes a `game`. Unlike tic-tac-toe, there is no natural concept of `end` here. Depending on our investment objective and horizon, we need to define the episode. It can be time-based, i.e. say a week of trading. It can be event-based, e.g. hitting a predefined profit or stoploss. There are other possibilities too.\n",
    "\n",
    "For a more complete discussion on reinforcement learning in trading, check out the upcoming course on reinforcement learning on [Quantra](https://quantra.quantinsti.com/) \n",
    "\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "- [1] from upcoming Reinforcement Learning course on [Quantra](https://quantra.quantinsti.com/)\n",
    "- [2]. All images sourced from google\n",
    "- [3]. Disclaimer: [Quantra](https://quantra.quantinsti.com/) is from [Quantinsti](https://www.quantinsti.com/), where I work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
